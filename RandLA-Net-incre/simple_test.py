import os
import re
import numpy as np
from helper_tool import DataProcessing as DP
from helper_tool import ConfigSemanticKITTI as cfg
from os.path import join
import os, pickle
import torch
import collections
from skimage import io
from skimage.transform import resize
import cv2
from tqdm import tqdm
from matplotlib import pyplot as plt
# p = 'D:/dataset/semantic-kitti/dataset/sequences_0.06\\05\\velodyne\\000360.npy'
# p = p.replace('\\', '/')
# # seq_id = re.split(r'/ ', p)
# seq_id = p.split('/')
# print(seq_id)
def get_data(file_path, label_path):
    kd_tree_path = file_path
    # Read pkl with search tree
    with open(kd_tree_path, 'rb') as f:
        search_tree = pickle.load(f)
    points = np.array(search_tree.data, copy=False)
    # Load labels
    if int(seq_id) >= 11:
        labels = np.zeros(np.shape(points)[0], dtype=np.uint8)
    else:
        labels = np.squeeze(np.load(label_path))
    return points, search_tree, labels

def crop_pc(points, labels, search_tree, pick_idx):
    # crop a fixed size point cloud for training
    center_point = points[pick_idx, :].reshape(1, -1)
    select_idx = search_tree.query(center_point, k=cfg.num_points)[1][0]
    select_idx = DP.shuffle_idx(select_idx)
    select_points = points[select_idx]
    select_labels = labels[select_idx]
    return select_points, select_labels, select_idx

def load_pc(pc_path, label_path):
    pc, tree, labels = get_data(pc_path, label_path)
    # crop a small point cloud
    pick_idx = np.random.choice(len(pc), 1)
    selected_pc, selected_labels, selected_idx = crop_pc(pc, labels, tree, pick_idx)
    return selected_pc.astype(np.float32), selected_labels.astype(np.int32), selected_idx.astype(np.int32)

def load_calib(calib_path):
    calib_filename = os.path.join(calib_path)
    return Calibration(calib_filename)

class Calibration(object):
    """ Calibration matrices and utils
        3d XYZ in <label>.txt are in rect camera coord.
        2d box xy are in image2 coord
        Points in <lidar>.bin are in Velodyne coord.

        y_image2 = P^2_rect * x_rect
        y_image2 = P^2_rect * R0_rect * Tr_velo_to_cam * x_velo
        x_ref = Tr_velo_to_cam * x_velo
        x_rect = R0_rect * x_ref

        P^2_rect = [f^2_u,  0,      c^2_u,  -f^2_u b^2_x;
                    0,      f^2_v,  c^2_v,  -f^2_v b^2_y;
                    0,      0,      1,      0]
                 = K * [1|t]

        image2 coord:
         ----> x-axis (u)
        |
        |
        v y-axis (v)

        velodyne coord:
        front x, left y, up z

        rect/ref camera coord:
        right x, down y, front z

        Ref (KITTI paper): http://www.cvlibs.net/publications/Geiger2013IJRR.pdf

        TODO(rqi): do matrix multiplication only once for each projection.
    """

    def __init__(self, calib_filepath):

        calibs = self.read_calib_file(calib_filepath)
        # Projection matrix from rect camera coord to image2 coord
        self.P = calibs["P2"]
        self.P = np.reshape(self.P, [3, 4])
        self.P3 = calibs["P3"]
        self.P3 = np.reshape(self.P3, [3, 4])
        # Rigid transform from Velodyne coord to reference camera coord
        self.V2C = calibs["Tr"]
        self.V2C = np.reshape(self.V2C, [3, 4])
        # self.C2V = inverse_rigid_trans(self.V2C)
        # Rotation from reference camera coord to rect camera coord
        # self.R0 = calibs["R0_rect"]
        # self.R0 = np.reshape(self.R0, [3, 3])

        # Camera intrinsics and extrinsics
        # self.c_u = self.P[0, 2]
        # self.c_v = self.P[1, 2]
        # self.f_u = self.P[0, 0]
        # self.f_v = self.P[1, 1]
        # self.b_x = self.P[0, 3] / (-self.f_u)  # relative
        # self.b_y = self.P[1, 3] / (-self.f_v)

    def read_calib_file(self, filepath):
        """ Read in a calibration file and parse into a dictionary.
        Ref: https://github.com/utiasSTARS/pykitti/blob/master/pykitti/utils.py
        """
        data = {}
        with open(filepath, "r") as f:
            for line in f.readlines():
                line = line.rstrip()
                if len(line) == 0:
                    continue
                key, value = line.split(":", 1)
                # The only non-float values in these files are dates, which
                # we don't care about anyway
                try:
                    data[key] = np.array([float(x) for x in value.split()])
                except ValueError:
                    pass

        return data

    def cart2hom(self, pts_3d):
        """ Input: nx3 points in Cartesian
            Oupput: nx4 points in Homogeneous by pending 1
        """
        n = pts_3d.shape[0]
        pts_3d_hom = np.hstack((pts_3d, np.ones((n, 1))))
        return pts_3d_hom

    # ===========================
    # ------- 3d to 3d ----------
    # ===========================
    def project_velo_to_ref(self, pts_3d_velo):
        pts_3d_velo = self.cart2hom(pts_3d_velo)  # nx4
        return np.dot(pts_3d_velo, np.transpose(self.V2C))

    # def project_ref_to_velo(self, pts_3d_ref):
    #     pts_3d_ref = self.cart2hom(pts_3d_ref)  # nx4
    #     return np.dot(pts_3d_ref, np.transpose(self.C2V))
    #
    # def project_rect_to_ref(self, pts_3d_rect):
    #     """ Input and Output are nx3 points """
    #     return np.transpose(np.dot(np.linalg.inv(self.R0), np.transpose(pts_3d_rect)))
    #
    # def project_ref_to_rect(self, pts_3d_ref):
    #     """ Input and Output are nx3 points """
    #     return np.transpose(np.dot(self.R0, np.transpose(pts_3d_ref)))

    # def project_rect_to_velo(self, pts_3d_rect):
    #     """ Input: nx3 points in rect camera coord.
    #         Output: nx3 points in velodyne coord.
    #     """
    #     pts_3d_ref = self.project_rect_to_ref(pts_3d_rect)
    #     return self.project_ref_to_velo(pts_3d_ref)

    def project_velo_to_rect(self, pts_3d_velo):
        pts_3d_ref = self.project_velo_to_ref(pts_3d_velo)
        return pts_3d_ref

    # ===========================
    # ------- 3d to 2d ----------
    # ===========================
    def project_rect_to_image(self, pts_3d_rect):
        """ Input: nx3 points in rect camera coord.
            Output: nx2 points in image2 coord.
        """
        pts_3d_rect = self.cart2hom(pts_3d_rect)
        pts_2d = np.dot(pts_3d_rect, np.transpose(self.P))  # nx3
        pts_2d[:, 0] /= pts_2d[:, 2]
        pts_2d[:, 1] /= pts_2d[:, 2]
        return pts_2d[:, 0:2]

    def project_velo_to_image(self, pts_3d_velo):
        """ Input: nx3 points in velodyne coord.
            Output: nx2 points in image2 coord.
        """
        pts_3d_rect = self.project_velo_to_rect(pts_3d_velo)
        return self.project_rect_to_image(pts_3d_rect)

    def project_8p_to_4p(self, pts_2d):
        x0 = np.min(pts_2d[:, 0])
        x1 = np.max(pts_2d[:, 0])
        y0 = np.min(pts_2d[:, 1])
        y1 = np.max(pts_2d[:, 1])
        x0 = max(0, x0)
        # x1 = min(x1, proj.image_width)
        y0 = max(0, y0)
        # y1 = min(y1, proj.image_height)
        return np.array([x0, y0, x1, y1])

    def project_velo_to_4p(self, pts_3d_velo):
        """ Input: nx3 points in velodyne coord.
            Output: 4 points in image2 coord.
        """
        pts_2d_velo = self.project_velo_to_image(pts_3d_velo)
        return self.project_8p_to_4p(pts_2d_velo)

def show_lidar_on_image(pc_velo, img, calib, img_width, img_height):
    """ Project LiDAR points to image """
    img = np.copy(img)
    imgfov_pc_velo, pts_2d, fov_inds = get_lidar_in_image_fov(
        pc_velo, calib, 0, 0, img_width, img_height, True
    )
    imgfov_pts_2d = pts_2d[fov_inds, :]
    imgfov_pc_rect = calib.project_velo_to_rect(imgfov_pc_velo)

    import matplotlib.pyplot as plt

    cmap = plt.cm.get_cmap("hsv", 400)
    cmap = np.array([cmap(i) for i in range(400)])[:, :3] * 255

    for i in range(imgfov_pts_2d.shape[0]):
        depth = imgfov_pc_rect[i, 2]
        color = cmap[int(640.0 / depth), :]
        cv2.circle(
            img,
            (int(np.round(imgfov_pts_2d[i, 0])), int(np.round(imgfov_pts_2d[i, 1]))),
            2,
            color=tuple(color),
            # color = (255,0,0),
            thickness=-1,
        )
    # cv2.imshow("projection", img)
    return img

def get_lidar_in_image_fov(
    pc_velo, calib, xmin, ymin, xmax, ymax, return_more=False, clip_distance=2.0
):
    """ Filter lidar points, keep those in image FOV """
    pts_2d = calib.project_velo_to_image(pc_velo)
    fov_inds = (
        (pts_2d[:, 0] < xmax)
        & (pts_2d[:, 0] >= xmin)
        & (pts_2d[:, 1] < ymax)
        & (pts_2d[:, 1] >= ymin)
    )
    fov_inds = fov_inds & (pc_velo[:, 0] > clip_distance)
    imgfov_pc_velo = pc_velo[fov_inds, :]
    if return_more:
        return imgfov_pc_velo, pts_2d, fov_inds
    else:
        return imgfov_pc_velo

if __name__ == '__main__':
    # image_dir = 'D:/dataset/data_odometry_color/dataset/sequences'
    # pc_dir = 'D:/dataset/semantic-kitti/dataset/sequences_0.06'
    # image_dir = '/mnt/images/sequences'
    # pc_dir = '/mnt/sequences_0.06'
    image_dir = '/root/autodl-tmp/sequences_0.06'
    pc_dir = '/root/autodl-tmp/dataset/sequences'
    seq_id = '07'
    frame_id = 136
    pc_path = "%06d.pkl" % (frame_id)
    image_path = "%06d.png" % (frame_id)
    label_path = "%06d.npy" % (frame_id)

    # calib = load_calib(f'D:/dataset/data_odometry_color/dataset/sequences/{seq_id}/calib.txt')
    # selected_pc, selected_labels, selected_idx = load_pc(f"D:/dataset/semantic-kitti/dataset/sequences_0.06/{seq_id}/KDTree/{pc_path}", f"D:/dataset/semantic-kitti/dataset/sequences_0.06/{seq_id}/labels/{label_path}")
    # img = io.imread(f'D:/dataset/data_odometry_color/dataset/sequences/{seq_id}/image_2/{image_path}')
    # calib = load_calib(f'/mnt/images/sequences/{seq_id}/calib.txt')
    # selected_pc, selected_labels, selected_idx = load_pc(f"/mnt/sequences_0.06/{seq_id}/KDTree/{pc_path}", f"/mnt/sequences_0.06/{seq_id}/labels/{label_path}")
    # img = cv2.imread(f'/mnt/images/sequences/{seq_id}/image_2/{image_path}')
    calib = load_calib(f'/root/autodl-tmp/dataset/sequences/{seq_id}/calib.txt')
    selected_pc, selected_labels, selected_idx = load_pc(f"/root/autodl-tmp/sequences_0.06/{seq_id}/KDTree/{pc_path}", f"/root/autodl-tmp/sequences_0.06/{seq_id}/labels/{label_path}")
    img = io.imread(f'/root/autodl-tmp/dataset/sequences/{seq_id}/image_2/{image_path}')
    img = resize(img, (370, 1226)).astype(np.float32)
    img_width = img.shape[1]
    img_height = img.shape[0]
    img_lidar = show_lidar_on_image(selected_pc[:, :3], img, calib, img_width, img_height)
    # img_lidar = cv2.cvtColor(img_lidar, cv2.COLOR_BGR2RGB)

    fig_lidar = plt.figure(figsize=(14, 7))
    ax_lidar = fig_lidar.subplots()
    ax_lidar.imshow(img_lidar)
    plt.show()

    # print(torch.cuda.device_count())

    # seq_id = os.listdir(image_dir)
    # img_stat = collections.defaultdict(int)
    # lidar_stat = collections.defaultdict(int)
    # for id in seq_id:
    #     print(f"now for sequent {id}")
    #     images_path = os.path.join(image_dir,id+'/image_2')
    #     img_files = os.listdir(images_path)
    #     img_stat[id] = len(img_files)
    #     lid_path = os.path.join(pc_dir,id+'/velodyne')
    #     lid_files = os.listdir(lid_path)
    #     lidar_stat[id] = len(lid_files)
    # print(img_stat)
    # print(lidar_stat)

